{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Tumor Classification \n",
    "\n",
    "## This project builds a machine learning model to predict if a tumor is malignant or benign using the Breast Cancer Wisconsin Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset and print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0  ...          17.33           184.60      2019.0            0.1622   \n",
      "1  ...          23.41           158.80      1956.0            0.1238   \n",
      "2  ...          25.53           152.50      1709.0            0.1444   \n",
      "3  ...          26.50            98.87       567.7            0.2098   \n",
      "4  ...          16.67           152.20      1575.0            0.1374   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0             0.6656           0.7119                0.2654          0.4601   \n",
      "1             0.1866           0.2416                0.1860          0.2750   \n",
      "2             0.4245           0.4504                0.2430          0.3613   \n",
      "3             0.8663           0.6869                0.2575          0.6638   \n",
      "4             0.2050           0.4000                0.1625          0.2364   \n",
      "\n",
      "   fractal_dimension_worst  Unnamed: 32  \n",
      "0                  0.11890          NaN  \n",
      "1                  0.08902          NaN  \n",
      "2                  0.08758          NaN  \n",
      "3                  0.17300          NaN  \n",
      "4                  0.07678          NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
      "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
      "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
      "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
      "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
      "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
      "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
      "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
      "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
      "\n",
      "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "count       569.000000        569.000000      569.000000           569.000000   \n",
      "mean          0.096360          0.104341        0.088799             0.048919   \n",
      "std           0.014064          0.052813        0.079720             0.038803   \n",
      "min           0.052630          0.019380        0.000000             0.000000   \n",
      "25%           0.086370          0.064920        0.029560             0.020310   \n",
      "50%           0.095870          0.092630        0.061540             0.033500   \n",
      "75%           0.105300          0.130400        0.130700             0.074000   \n",
      "max           0.163400          0.345400        0.426800             0.201200   \n",
      "\n",
      "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
      "count     569.000000  ...     569.000000       569.000000   569.000000   \n",
      "mean        0.181162  ...      25.677223       107.261213   880.583128   \n",
      "std         0.027414  ...       6.146258        33.602542   569.356993   \n",
      "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
      "25%         0.161900  ...      21.080000        84.110000   515.300000   \n",
      "50%         0.179200  ...      25.410000        97.660000   686.500000   \n",
      "75%         0.195700  ...      29.720000       125.400000  1084.000000   \n",
      "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
      "\n",
      "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "count        569.000000         569.000000       569.000000   \n",
      "mean           0.132369           0.254265         0.272188   \n",
      "std            0.022832           0.157336         0.208624   \n",
      "min            0.071170           0.027290         0.000000   \n",
      "25%            0.116600           0.147200         0.114500   \n",
      "50%            0.131300           0.211900         0.226700   \n",
      "75%            0.146000           0.339100         0.382900   \n",
      "max            0.222600           1.058000         1.252000   \n",
      "\n",
      "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
      "count            569.000000      569.000000               569.000000   \n",
      "mean               0.114606        0.290076                 0.083946   \n",
      "std                0.065732        0.061867                 0.018061   \n",
      "min                0.000000        0.156500                 0.055040   \n",
      "25%                0.064930        0.250400                 0.071460   \n",
      "50%                0.099930        0.282200                 0.080040   \n",
      "75%                0.161400        0.317900                 0.092080   \n",
      "max                0.291000        0.663800                 0.207500   \n",
      "\n",
      "       Unnamed: 32  \n",
      "count          0.0  \n",
      "mean           NaN  \n",
      "std            NaN  \n",
      "min            NaN  \n",
      "25%            NaN  \n",
      "50%            NaN  \n",
      "75%            NaN  \n",
      "max            NaN  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('BreastCancerDataset.csv')\n",
    "print(features.head(5))\n",
    "print(features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Binary Values\n",
    "\n",
    "In Machine Learning, we like to have our predictions based on binary values (0 and 1). Currently, the diagnosis attribute is labeled as 'M' or 'B' as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     M\n",
      "1     M\n",
      "2     M\n",
      "3     M\n",
      "4     M\n",
      "5     M\n",
      "6     M\n",
      "7     M\n",
      "8     M\n",
      "9     M\n",
      "10    M\n",
      "11    M\n",
      "12    M\n",
      "13    M\n",
      "14    M\n",
      "15    M\n",
      "16    M\n",
      "17    M\n",
      "18    M\n",
      "19    B\n",
      "20    B\n",
      "21    B\n",
      "22    M\n",
      "23    M\n",
      "24    M\n",
      "25    M\n",
      "26    M\n",
      "27    M\n",
      "28    M\n",
      "29    M\n",
      "Name: diagnosis, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(features['diagnosis'].head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fix this, we can use the get_dummies method to convert categorical values to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.get_dummies(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two columns: diagnosis_M, where values of 1 represent that the tumor is malignant and 0 if it is not, and diagnosis_B where values of 1 represent that the tumor is benign and 0 if it is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    1\n",
      "12    1\n",
      "13    1\n",
      "14    1\n",
      "15    1\n",
      "16    1\n",
      "17    1\n",
      "18    1\n",
      "19    0\n",
      "Name: diagnosis_M, dtype: uint8\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    1\n",
      "Name: diagnosis_B, dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "print(features['diagnosis_M'].head(20))\n",
    "print(features['diagnosis_B'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "\n",
    "Next, we need to split our data so that we identify the variables that we are trying to predict (labels) and the data that will help us get to the prediction (features).  \n",
    "\n",
    "**Labels** typically represent one column in the data and **features** represent most, if not all the other columns. In this case, we are trying to predict diagnosis_M, so our labels will be the diagnosis_M column in the data.\n",
    "\n",
    "Next, we need to determine the features we will use to predict the **diagnosis_M** variable. Since the features represent most of the other columns in the data, we just need to remove the uncessessary and redundant columns. This includes the diagnosis_M column itself since it is what we are trying to predict, and the diagnosis_B column since it can be used as another label and we do not want it to be part of our model. The 'id' variable will also be removed because it will not help us in our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(features['diagnosis_M'])\n",
    "\n",
    "features = features.drop('diagnosis_M', axis = 1)\n",
    "features = features.drop('diagnosis_B', axis = 1)\n",
    "features = features.drop('id', axis = 1)\n",
    "features = features.drop('Unnamed: 32', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see all the features will be using in our machine learning model. Again, these are all the variables that will help us predict if the diagnosis is malignant or benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(features.columns)\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and testing sets\n",
    "\n",
    " We will split the data into two: \n",
    " - 80% will be used to train the model\n",
    " - 20% will be used to test the model\n",
    "    \n",
    "train_features represents the feature columns of the training data. It will be used to predict train_labels (the diagnosis_M column)\n",
    "\n",
    "The machine learning model will use test_features to build predictions. These predictions will be compared to the test_labels to test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should print evaluate the size of our training and testing sets. By using the shape method, we are able to tell the number of rows and columns that represent each. \n",
    "\n",
    "The training set is comprised of 455 rows and the testing set is comprised of 114 rows. These numbers can be changed by changing the test_size variable in the cell above. The greater the test_size, the more rows the testing set will have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (455, 30)\n",
      "Training Labels Shape: (455,)\n",
      "Testing Features Shape: (114, 30)\n",
      "Testing Labels Shape: (114,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print our train features and train labels to see what we are dealing with. This is the data that we will feed into the machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.029e+00 1.733e+01 5.879e+01 ... 1.750e-01 4.228e-01 1.175e-01]\n",
      " [2.109e+01 2.657e+01 1.427e+02 ... 2.903e-01 4.098e-01 1.284e-01]\n",
      " [9.173e+00 1.386e+01 5.920e+01 ... 5.087e-02 3.282e-01 8.490e-02]\n",
      " ...\n",
      " [1.429e+01 1.682e+01 9.030e+01 ... 3.333e-02 2.458e-01 6.120e-02]\n",
      " [1.398e+01 1.962e+01 9.112e+01 ... 1.827e-01 3.179e-01 1.055e-01]\n",
      " [1.218e+01 2.052e+01 7.722e+01 ... 7.431e-02 2.694e-01 6.878e-02]]\n",
      "\n",
      "[0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1\n",
      " 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0\n",
      " 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1\n",
      " 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
      " 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0\n",
      " 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1\n",
      " 0 1 1 0 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_features)\n",
    "print()\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Machine Learning Model\n",
    "\n",
    "I have chosen to use the the Gaussian Naive Bayes Classification model because I have found it to have the highest accuracy compared to to other models. Other classication models will be shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize naive bayes model\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Form predictions on the test data \n",
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model was trained with the training data, it was given test_features to predict the diagnosis for each of those cases. The predictions of the model will be compared to test_labels, since they represent the actual diagnosis for each case. \n",
    "\n",
    "Lets print test_labels and the model's predictions just to see what we are dealing with. Remember, since our labels represent the diagnosis_M variable, a 1 represents that the tumor is malignant and 0 if it is not, therefore meaning it is benign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1\n",
      " 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1]\n",
      "[0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)\n",
    "print(predictions)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "![confusionmatrix](https://user-images.githubusercontent.com/43652410/79499516-17bbf680-7ff9-11ea-8e3d-4e9210c99fa1.jpg)
"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Accuracy\n",
    "\n",
    "Now, lets check the accuracy of the model. The accuracy represents the correct predictions / the total amount of predictions. However, we can get more specific.\n",
    "\n",
    "We will be using a confusion matrix to further gauge the model's performance. \n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 97.36842105263158%\n",
      "[[71  0]\n",
      " [ 3 40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        71\n",
      "           1       1.00      0.93      0.96        43\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       114\n",
      "   macro avg       0.98      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nAccuracy:', str(accuracy_score(test_labels, predictions) * 100) + '%')\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have achieved 97% accuracy with our model! \n",
    "\n",
    "According to the confusion matrix, the model predicted:\n",
    "- 71 True Positives\n",
    "- 3 False Positives\n",
    "- 0 False Negatives\n",
    "- 40 True Negatives\n",
    "\n",
    "Definition of the terms:\n",
    "- True Positive (TP) : Observation is positive, and is predicted to be positive.\n",
    "- False Positive (FP) : Observation is negative, but is predicted positive.\n",
    "- False Negative (FN) : Observation is positive, but is predicted negative.\n",
    "- True Negative (TN) : Observation is negative, and is predicted to be negative.\n",
    "\n",
    "\n",
    "More info on model performance can be found here: \n",
    "https://www.geeksforgeeks.org/confusion-matrix-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "In order to better determine the accuracy of the model, we will use 10 fold cross-validation. It ensures that every observation from the original dataset has the chance of appearing in training and test set. This is one of the best approaches if we have a limited input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Accuracy: 93.86796733212338%\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, features, labels, scoring='accuracy', cv=10)\n",
    "print('\\nMean Accuracy:', str(np.mean(scores) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info on cross validation can be found here: \n",
    "https://machinelearningmastery.com/k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing other Machine Learning Models\n",
    "\n",
    "In machine learning, it is important to test other algorithms to see which model results in the highest accuracy. \n",
    "Here are the results of several other classification algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faiza\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\faiza\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Accuracy: 95.6140350877193%\n",
      "[[70  1]\n",
      " [ 4 39]]\n",
      "\n",
      "Support Vector Machine Accuracy: 90.35087719298247%\n",
      "[[71  0]\n",
      " [11 32]]\n",
      "\n",
      "Neural Network Accuracy: 94.73684210526315%\n",
      "[[70  1]\n",
      " [ 5 38]]\n",
      "\n",
      "Random Forest Accuracy: 96.49122807017544%\n",
      "[[70  1]\n",
      " [ 3 40]]\n",
      "\n",
      "KNeighbors Accuracy: 92.98245614035088%\n",
      "[[68  3]\n",
      " [ 5 38]]\n",
      "\n",
      "Linear Discriminant Analysis Accuracy: 95.6140350877193%\n",
      "[[70  1]\n",
      " [ 4 39]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr.fit(train_features, train_labels)\n",
    "predictions = lr.predict(test_features)\n",
    "\n",
    "print('\\nLogistic Regression Accuracy:', str(accuracy_score(test_labels, predictions) * 100) + '%')\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "\n",
    "\n",
    "SVM = svm.LinearSVC(max_iter=1000)\n",
    "SVM.fit(train_features, train_labels)\n",
    "predictions = SVM.predict(test_features)\n",
    "\n",
    "print('\\nSupport Vector Machine Accuracy:', str(accuracy_score(test_labels, predictions) * 100) + '%')\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "\n",
    "\n",
    "neuralnNet = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(15,10,5),max_iter=2000)\n",
    "neuralnNet.fit(train_features, train_labels)\n",
    "predictions = neuralnNet.predict(test_features)\n",
    "\n",
    "print('\\nNeural Network Accuracy:', str(accuracy_score(test_labels, predictions) * 100) + '%')\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(train_features, train_labels)\n",
    "predictions = rf.predict(test_features)\n",
    "\n",
    "print('\\nRandom Forest Accuracy:', str(accuracy_score(test_labels, predictions) * 100) + '%')\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_features, train_labels)\n",
    "predictions = knn.predict(test_features)\n",
    "\n",
    "print('\\nKNeighbors Accuracy:', str(accuracy_score(test_labels, predictions) * 100) + '%')\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(solver='svd')\n",
    "lda.fit(train_features, train_labels)\n",
    "predictions = lda.predict(test_features)\n",
    "\n",
    "print('\\nLinear Discriminant Analysis Accuracy:', str(accuracy_score(test_labels, predictions) * 100) + '%')\n",
    "print(confusion_matrix(test_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
